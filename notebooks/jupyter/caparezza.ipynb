{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8848cb",
   "metadata": {},
   "source": [
    "# Caparezza Sentiment Analysis\n",
    "\n",
    "In this notebook, a text mining process on the works of the Italian singer\n",
    "**Caparezza** will be performed, with the intention of finding recurring themes\n",
    "throughout his albums and songs.\n",
    "\n",
    "Much of the inspiration for this project comes from [this repository](https://tm4ss.github.io/docs/index.html).\n",
    "\n",
    "## Get the lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ff17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(geniusr)\n",
    "library(tidyverse)\n",
    "library(tidytext)\n",
    "library(quanteda)\n",
    "library(quanteda.textstats)\n",
    "library(udpipe)\n",
    "library(wordcloud)\n",
    "library(textdata)\n",
    "library(reshape2)\n",
    "library(igraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f7d53e",
   "metadata": {},
   "source": [
    "Lyrics for the songs can be obtained using the *geniusr* package, available\n",
    "at https://github.com/ewenme/geniusr. Follow the instructions at that link\n",
    "to set up an API key and use it to query the Genius lyrics database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63790234",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sys.setenv(GENIUS_API_TOKEN = \"mjJVgXu4_yQNgYXFYV72Q9uXCIdTFFq3s3c8PPcUT6G1tlAedbpg0dpdZv6KJQW7\") \n",
    "\n",
    "artist_id <- search_artist(\"Caparezza\")$artist_id\n",
    "artist_songs <- get_artist_songs(artist_id)\n",
    "\n",
    "songs_ids <- c()\n",
    "for (song in artist_songs$content) {\n",
    "  song_id <- song$id\n",
    "  songs_ids <- songs_ids %>% append(song_id)\n",
    "}\n",
    "\n",
    "songs_titles <- c()\n",
    "songs_albums <- c()\n",
    "songs_lyrics <- c()\n",
    "for (i in 1:length(songs_ids)) {\n",
    "  cat(\"Getting\", i, \"of\", length(songs_ids), \"id:\", songs_ids[i], \"\\n\")\n",
    "  song <- get_song(songs_ids[i])$content\n",
    "  song_title <- song$title\n",
    "  song_album <- song$album$name\n",
    "  song_lyrics <- list(get_lyrics_id(songs_ids[i]))\n",
    "  # some songs have no album associated with them, so we won't consider them\n",
    "  # additionally, some songs have no lyrics, we are excluding them too\n",
    "  if (!is.null(song_album) & nrow(song_lyrics[[1]]) != 0){\n",
    "    songs_titles <- songs_titles %>% append(song_title)\n",
    "    songs_albums <- songs_albums %>% append(song_album)\n",
    "    songs_lyrics <- songs_lyrics %>% append(song_lyrics) \n",
    "  }\n",
    "}\n",
    "\n",
    "# collapse all lyrics lines into a single text\n",
    "for (i in 1:length(songs_lyrics)){\n",
    "  songs_lyrics[[i]] <- songs_lyrics[[i]]$line %>% paste(collapse = \"\\n\")\n",
    "}\n",
    "\n",
    "songs_lyrics <- unlist(songs_lyrics)\n",
    "\n",
    "songs <- data.frame(title = songs_titles, album = songs_albums, lyrics = songs_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9226c8c0",
   "metadata": {},
   "source": [
    "Now we have got a dataframe with data about each individual song.\n",
    "Let's look at the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11580e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs %>% head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463207a",
   "metadata": {},
   "source": [
    "Songs are ordered alphabetically by the title, and the first one appears\n",
    "to be *Abiura Di Me*.\n",
    "\n",
    "Let's do some cleaning: we are going to ignore alternative versions of the same\n",
    "song, such as live versions, radio edits and remixes.\n",
    "We are also keeping only the main albums, ignoring specials or compilations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "albums <- c(\"?! (Caparezza ?!)\", \"VeritÃ  Supposte\", \"Habemus Capa\",\n",
    "            \"Le Dimensioni Del Mio Caos\", \"Il Sogno Eretico\", \"Museica\",\n",
    "            \"Prisoner 709\", \"Exuvia\")\n",
    "songs <- songs %>%\n",
    "  filter(!grepl(\"Live|Radio Edit|Radio Version|Remix|RMX\", title) & album %in% albums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2079757",
   "metadata": {},
   "source": [
    "Finally, we order songs chronologically by the album, then add an id for each song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs <- songs %>% arrange(match(album, albums), title)\n",
    "doc_ids <- vector()\n",
    "for(i in 1:nrow(songs)){\n",
    "  id <- paste(\"doc\", toString(i), sep = \"\")\n",
    "  doc_ids <- doc_ids %>% append(id)\n",
    "}\n",
    "songs <- songs %>% mutate(doc_id = doc_ids, .before = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f7010",
   "metadata": {},
   "source": [
    "Let's check the number of songs for each album."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519ad545",
   "metadata": {},
   "outputs": [],
   "source": [
    "table(songs$album) %>% as.data.frame() %>% arrange(-Freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89bc23a",
   "metadata": {},
   "source": [
    "The frequencies are correct, as every Caparezza fan will recognize.\n",
    "\n",
    "## Text pre-processing\n",
    "\n",
    "Now we start with some text pre-processing: let's create a corpus from the\n",
    "songs' lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca526dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "caparezza_corpus <- corpus(songs$lyrics, docnames = songs$id)\n",
    "summary(caparezza_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e896143",
   "metadata": {},
   "source": [
    "There is a total of 129 documents and for each one the total number of tokens\n",
    "is displayed. A *token* is a single occurrence of a word in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0817b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(as.character(caparezza_corpus[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719ec1b",
   "metadata": {},
   "source": [
    "The first document of the corpus is the song \"Cammina Solo\" from *?!*, since\n",
    "we have ordered songs by album and then by title.\n",
    "\n",
    "One of the main steps of lexical analysis is the removal of punctuation marks,\n",
    "numbers and symbols which are not useful towards the text interpretation.\n",
    "\n",
    "The *quanteda* package offers some useful tools for these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb8a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens <- caparezza_corpus %>%\n",
    "  quanteda::tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%\n",
    "  tokens_tolower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1980110",
   "metadata": {},
   "source": [
    "Additionally, we would like to lemmatise: that is, to consider the *lemma* from\n",
    "which a word originates, instead of the world itself. For example, the\n",
    "infinitive forms of the verbs or the standard singular masculine adjective form\n",
    "are sufficient to express a concept, so we can safely use those instead of their\n",
    "derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948952cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt <- sapply(corpus_tokens, FUN=function(x) paste(x, collapse = \"\\n\"))\n",
    "udpipe_download_model(language = \"italian-isdt\", model_dir = \"resources/\")\n",
    "lang_model <- udpipe_load_model(file = \"resources/italian-isdt-ud-2.5-191206.udpipe\")\n",
    "outL <- udpipe_annotate(lang_model, x = txt, tokenizer = \"vertical\", trace = TRUE) %>%\n",
    "  as.data.frame()\n",
    "it_stopwords <- readLines(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-it/master/stopwords-it.txt\")\n",
    "outL <- outL %>% filter(!(token %in% it_stopwords) & !(lemma %in% it_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e3343",
   "metadata": {},
   "source": [
    "We have made use of the *UDPipe* library annotation function.\n",
    "\n",
    "In the process, we have also removed *stopwords*, words that do not bring\n",
    "any meaningful addition to our texts, but are only necessary to connect other\n",
    "words and respect syntactic rules.\n",
    "\n",
    "Let's have a look at random sample of five elements of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf07e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "outL %>% select(doc_id, token, lemma, upos) %>% sample_n(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b5ae7c",
   "metadata": {},
   "source": [
    "To further enhance our analysis, let's focus only on nouns, proper nouns,\n",
    "adjectives and verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8778be57",
   "metadata": {},
   "outputs": [],
   "source": [
    "outL_reduced <- outL %>% filter(upos %in% c(\"NOUN\", \"PROPN\", \"ADJ\", \"VERB\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f7763",
   "metadata": {},
   "source": [
    "Now we create a new corpus with the lemmatized lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cd684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fct_inorder preserves original order of the column\n",
    "lemmatized_lyrics <- outL_reduced %>% group_by(doc_id = fct_inorder(doc_id)) %>%\n",
    "  summarise(lemmatized = paste(lemma, collapse = \" \"))\n",
    "songs <- songs %>% right_join(lemmatized_lyrics, by = \"doc_id\")\n",
    "\n",
    "caparezza_corpus <- songs$lemmatized %>% corpus(docnames = songs$id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bdf296",
   "metadata": {},
   "source": [
    "One final step for text-processing consists in handling collocations, or\n",
    "multi-word units(MWUs). A collocation is a set of two or more\n",
    "words which are closely related and are often used together to express a single\n",
    "concept.\n",
    "\n",
    "They can be identified through statistical, objective methods, like in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collocations <- caparezza_corpus %>% tokens() %>% textstat_collocations %>%\n",
    "  arrange(-count) %>% head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6273e2fa",
   "metadata": {},
   "source": [
    "Looking at the collocations found by the function, we decide to take action on\n",
    "two of them, which seem to be clearly meant to be used together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ba9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM <- caparezza_corpus %>% tokens() %>% tokens_compound(collocations[c(4,6),]) %>%\n",
    "  tokens_remove(\"\") %>% dfm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd2221",
   "metadata": {},
   "source": [
    "## Lexical Analysis\n",
    "\n",
    "In the last step, we have created an object called DTM. This is the document-term\n",
    "matrix of our corpus: it's a matrix which contains documents on its rows and\n",
    "terms on its columns. Every element of the matrix describes the number of\n",
    "occurrencies of the corresponding term inside the corresponding document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea291e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM\n",
    "DTM %>% dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd2367",
   "metadata": {},
   "source": [
    "We have got a total of 129 documents and 8679 unique terms in our corpus.\n",
    "\n",
    "Let's build a frequencies table for our terms and look at the most recurring ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words <- colnames(DTM)\n",
    "freqs <- colSums(DTM)\n",
    "wordlist <- data.frame(words, freqs)\n",
    "wordlist %>% arrange(-freqs) %>% head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9072b9fc",
   "metadata": {},
   "source": [
    "And now let's look at some useful quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size <- sum(wordlist$freqs)\n",
    "corpus_size\n",
    "\n",
    "vocabulary_size <- nrow(wordlist)\n",
    "vocabulary_size\n",
    "\n",
    "words_occurrencies <- wordlist %>% group_by(freqs) %>% summarise(vK = n()) %>% arrange(-vK)\n",
    "words_occurrencies\n",
    "\n",
    "lexicon_width <- vocabulary_size/corpus_size\n",
    "lexicon_width\n",
    "language_refinement <- words_occurrencies$vK[1] / colSums(words_occurrencies)[2]\n",
    "language_refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec338c29",
   "metadata": {},
   "source": [
    "Words that appear only once in the entire corpus are known as *hapaxes*.\n",
    "*Lexicon width* is the percentage of unique words in the total number of words\n",
    "used in the corpus.\n",
    "*Language refinement* is the percentage of hapaxes in the total number of words\n",
    "used in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e4fee",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "In order to visualize the most frequent terms, we can use a *word cloud*, which\n",
    "portrays the terms in the corpus with different sizes depending on their relative\n",
    "frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318cda23",
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mar=c(1,1,0.5,1))\n",
    "wordcloud(words = wordlist$words, freq = wordlist$freqs, scale = c(3.5, 0.35),\n",
    "          max.words = 50, random.order = F,\n",
    "          colors = RColorBrewer::brewer.pal(name = \"Dark2\", n = 4))\n",
    "text(0.5, 1, \"wordcloud with TF ponderation\", font = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254d23c",
   "metadata": {},
   "source": [
    "It could be interesting to know in which songs a given word appears, especially\n",
    "if it seems an odd one.\n",
    "We can use this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f537f92",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "count <- 1\n",
    "for(i in songs$lyrics){\n",
    "  if(grepl(\"mamma\", i)){\n",
    "    print(songs$title[[count]])\n",
    "  }\n",
    "  count <- count + 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a4d76",
   "metadata": {},
   "source": [
    "## TF-IDF ponderation\n",
    "\n",
    "It is often necessary to weight terms in the corpus on the basis of their relative\n",
    "importance. For example, a term which occurs frequently in a document has a strong\n",
    "relevance for that document, but a term which occurs frequently in many different\n",
    "documents is less informative for any single document and should be treated as\n",
    "less relevant.\n",
    "\n",
    "To account for this situation, we are going to use the term frequency-inverse\n",
    "document frequency (TF-IDF) weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2cf953",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf <- dfm_tfidf(DTM)\n",
    "freqs_tf_idf <- colSums(tf_idf)\n",
    "words_tf_idf <- colnames(tf_idf)\n",
    "wordlist_tf_idf <- data.frame(words = words_tf_idf, freqs = freqs_tf_idf)\n",
    "wordlist_tf_idf %>% arrange(-freqs) %>% head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72201997",
   "metadata": {},
   "source": [
    "These are the most common terms after the applied weighting.\n",
    "\n",
    "Let's have a look at the new word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a5caf6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "par(mar=c(1,1,0.5,1))\n",
    "wordcloud(words = wordlist_tf_idf$words, freq = wordlist_tf_idf$freqs,\n",
    "          scale = c(3.5, 0.35), max.words = 50, random.order = F,\n",
    "          colors = RColorBrewer::brewer.pal(name = \"Dark2\", n = 4))\n",
    "text(0.5, 1, \"wordcloud with TF-IDF ponderation\", font = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f82f1a",
   "metadata": {},
   "source": [
    "## Group by albums\n",
    "\n",
    "Let's redo some of our previous analysis, this time considering the entire albums\n",
    "as documents, rather than individual songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde0f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_lyrics_by_album <- songs %>% group_by(album) %>%\n",
    "  summarise(lemmatized = paste(lemmatized, collapse = \" \")) %>% arrange(match(album, albums))\n",
    "corpus_album <- lemmatized_lyrics_by_album$lemmatized %>% corpus()\n",
    "\n",
    "DTM_album <- corpus_album %>% tokens() %>% dfm()\n",
    "DTM_album\n",
    "docnames(DTM_album) <- lemmatized_lyrics_by_album$album\n",
    "wordlist_album <- DTM_album %>% as.matrix() %>% t()\n",
    "par(mar=c(0,0,0,0))\n",
    "comparison.cloud(wordlist_album, scale = c(2, 1), max.words = 50, random.order = F,\n",
    "                 title.size = 1, colors = RColorBrewer::brewer.pal(name = \"Dark2\", n = 8))\n",
    "text(0.5, 1, \"comparison cloud by album\", font = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868807e1",
   "metadata": {},
   "source": [
    "We can recognize some iconic songs from each album by looking at the words:\n",
    "*conflitto* from *Il Conflitto* (*?!*),\n",
    "*secessionista* from *Inno Verdano* (*Habemus Capa*),\n",
    "*campione* from *Campione dei Novanta* (*Exuvia*), and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4137d5",
   "metadata": {},
   "source": [
    "## Co-occurrence analysis\n",
    "\n",
    "Next, we are going to analyze the co-occurrence of words in order to find\n",
    "terms which tend to appear in the same documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793de52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "binDTM <- DTM %>% dfm_trim(min_docfreq = 10) %>% dfm_weight(\"boolean\")\n",
    "coocCounts <- t(binDTM) %*% binDTM\n",
    "as.matrix(coocCounts[100:102, 100:102])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee59f73e",
   "metadata": {},
   "source": [
    "For example, the words *libro* and *leggere* appear together in four documents.\n",
    "The diagonal elements of the matrix are the total occurrencies of the word.\n",
    "\n",
    "Let's calculate some co-occurrence measurements for the words *lavoro* and then\n",
    "*leggere*: Mutual Information, Dice, and Log-Likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coocTerm <- \"lavoro\"\n",
    "k <- nrow(binDTM)\n",
    "ki <- sum(binDTM[, coocTerm])\n",
    "kj <- colSums(binDTM)\n",
    "names(kj) <- colnames(binDTM)\n",
    "kij <- coocCounts[coocTerm, ]\n",
    "\n",
    "mutualInformationSig <- log(k * kij / (ki * kj))\n",
    "mutualInformationSig <- mutualInformationSig[order(mutualInformationSig, decreasing = TRUE)]\n",
    "\n",
    "dicesig <- 2 * kij / (ki + kj)\n",
    "dicesig <- dicesig[order(dicesig, decreasing=TRUE)]\n",
    "\n",
    "logsig <- 2 * ((k * log(k)) - (ki * log(ki)) - (kj * log(kj)) + (kij * log(kij)) \n",
    "               + (k - ki - kj + kij) * log(k - ki - kj + kij) \n",
    "               + (ki - kij) * log(ki - kij) + (kj - kij) * log(kj - kij) \n",
    "               - (k - ki) * log(k - ki) - (k - kj) * log(k - kj))\n",
    "logsig <- logsig[order(logsig, decreasing=T)]\n",
    "\n",
    "resultOverView <- data.frame(\n",
    "  names(sort(kij, decreasing=T)[1:10]), sort(kij, decreasing=T)[1:10],\n",
    "  names(mutualInformationSig[1:10]), mutualInformationSig[1:10], \n",
    "  names(dicesig[1:10]), dicesig[1:10], \n",
    "  names(logsig[1:10]), logsig[1:10],\n",
    "  row.names = NULL)\n",
    "colnames(resultOverView) <- c(\"Freq-terms\", \"Freq\", \"MI-terms\", \"MI\", \"Dice-Terms\", \"Dice\", \"LL-Terms\", \"LL\")\n",
    "print(resultOverView)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8042c85",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "coocTerm <- \"leggere\"\n",
    "k <- nrow(binDTM)\n",
    "ki <- sum(binDTM[, coocTerm])\n",
    "kj <- colSums(binDTM)\n",
    "names(kj) <- colnames(binDTM)\n",
    "kij <- coocCounts[coocTerm, ]\n",
    "\n",
    "mutualInformationSig <- log(k * kij / (ki * kj))\n",
    "mutualInformationSig <- mutualInformationSig[order(mutualInformationSig, decreasing = TRUE)]\n",
    "\n",
    "dicesig <- 2 * kij / (ki + kj)\n",
    "dicesig <- dicesig[order(dicesig, decreasing=TRUE)]\n",
    "\n",
    "logsig <- 2 * ((k * log(k)) - (ki * log(ki)) - (kj * log(kj)) + (kij * log(kij)) \n",
    "               + (k - ki - kj + kij) * log(k - ki - kj + kij) \n",
    "               + (ki - kij) * log(ki - kij) + (kj - kij) * log(kj - kij) \n",
    "               - (k - ki) * log(k - ki) - (k - kj) * log(k - kj))\n",
    "logsig <- logsig[order(logsig, decreasing=T)]\n",
    "\n",
    "resultOverView <- data.frame(\n",
    "  names(sort(kij, decreasing=T)[1:10]), sort(kij, decreasing=T)[1:10],\n",
    "  names(mutualInformationSig[1:10]), mutualInformationSig[1:10], \n",
    "  names(dicesig[1:10]), dicesig[1:10], \n",
    "  names(logsig[1:10]), logsig[1:10],\n",
    "  row.names = NULL)\n",
    "colnames(resultOverView) <- c(\"Freq-terms\", \"Freq\", \"MI-terms\", \"MI\", \"Dice-Terms\", \"Dice\", \"LL-Terms\", \"LL\")\n",
    "print(resultOverView)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca862b",
   "metadata": {},
   "source": [
    "## Co-occurrence visualization\n",
    "\n",
    "To visualize co-occurrence for a single term, we may use a graph which displays\n",
    "the terms co-occurrence network (including secondary co-occurrence levels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093b4a5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "source(\"resources/calculateCoocStatistics.R\")\n",
    "numberOfCoocs <- 10\n",
    "coocTerm <- \"libro\"\n",
    "coocs <- calculateCoocStatistics(coocTerm, binDTM, measure=\"LOGLIK\")\n",
    "print(coocs[1:numberOfCoocs])\n",
    "resultGraph <- data.frame(from = character(), to = character(), sig = numeric(0))\n",
    "tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))\n",
    "\n",
    "# Fill the data.frame to produce the correct number of lines\n",
    "tmpGraph[1:numberOfCoocs, 3] <- coocs[1:numberOfCoocs]\n",
    "# Entry of the search word into the first column in all lines\n",
    "tmpGraph[, 1] <- coocTerm\n",
    "# Entry of the co-occurrences into the second column of the respective line\n",
    "tmpGraph[, 2] <- names(coocs)[1:numberOfCoocs]\n",
    "# Set the significances\n",
    "tmpGraph[, 3] <- coocs[1:numberOfCoocs]\n",
    "\n",
    "# Attach the triples to resultGraph\n",
    "resultGraph <- rbind(resultGraph, tmpGraph)\n",
    "\n",
    "# Iteration over the most significant numberOfCoocs co-occurrences of the search term\n",
    "for (i in 1:numberOfCoocs){\n",
    "  \n",
    "  # Calling up the co-occurrence calculation for term i from the search words co-occurrences\n",
    "  newCoocTerm <- names(coocs)[i]\n",
    "  coocs2 <- calculateCoocStatistics(newCoocTerm, binDTM, measure=\"LOGLIK\")\n",
    "  \n",
    "  #print the co-occurrences\n",
    "  coocs2[1:10]\n",
    "  \n",
    "  # Structure of the temporary graph object\n",
    "  tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))\n",
    "  tmpGraph[1:numberOfCoocs, 3] <- coocs2[1:numberOfCoocs]\n",
    "  tmpGraph[, 1] <- newCoocTerm\n",
    "  tmpGraph[, 2] <- names(coocs2)[1:numberOfCoocs]\n",
    "  tmpGraph[, 3] <- coocs2[1:numberOfCoocs]\n",
    "  \n",
    "  #Append the result to the result graph\n",
    "  resultGraph <- rbind(resultGraph, tmpGraph[2:length(tmpGraph[, 1]), ])\n",
    "}\n",
    "\n",
    "# Sample of some examples from resultGraph\n",
    "resultGraph[sample(nrow(resultGraph), 6), ]\n",
    "\n",
    "\n",
    "# set seed for graph plot\n",
    "set.seed(1)\n",
    "\n",
    "# Create the graph object as undirected graph\n",
    "graphNetwork <- graph.data.frame(resultGraph, directed = F)\n",
    "\n",
    "# Identification of all nodes with less than 2 edges\n",
    "verticesToRemove <- V(graphNetwork)[degree(graphNetwork) < 2]\n",
    "# These edges are removed from the graph\n",
    "graphNetwork <- delete.vertices(graphNetwork, verticesToRemove) \n",
    "\n",
    "# Assign colors to nodes (search term blue, others orange)\n",
    "V(graphNetwork)$color <- ifelse(V(graphNetwork)$name == coocTerm, 'cornflowerblue', 'orange') \n",
    "\n",
    "# Set edge colors\n",
    "E(graphNetwork)$color <- adjustcolor(\"DarkGray\", alpha.f = .5)\n",
    "# scale significance between 1 and 10 for edge width\n",
    "E(graphNetwork)$width <- scales::rescale(E(graphNetwork)$sig, to = c(1, 10))\n",
    "\n",
    "# Set edges with radius\n",
    "E(graphNetwork)$curved <- 0.15 \n",
    "# Size the nodes by their degree of networking (scaled between 5 and 15)\n",
    "V(graphNetwork)$size <- scales::rescale(log(degree(graphNetwork)), to = c(5, 15))\n",
    "\n",
    "# Define the frame and spacing for the plot\n",
    "par(mai=c(0,0,1,0)) \n",
    "\n",
    "# Final Plot\n",
    "plot(\n",
    "  graphNetwork,             \n",
    "  layout = layout.fruchterman.reingold, # Force Directed Layout \n",
    "  main = paste(coocTerm, ' Graph'),\n",
    "  vertex.label.family = \"sans\",\n",
    "  vertex.label.cex = 0.8,\n",
    "  vertex.shape = \"circle\",\n",
    "  vertex.label.dist = 0.5,          # Labels of the nodes moved slightly\n",
    "  vertex.frame.color = adjustcolor(\"darkgray\", alpha.f = .5),\n",
    "  vertex.label.color = 'black',     # Color of node names\n",
    "  vertex.label.font = 2,            # Font of node names\n",
    "  vertex.label = V(graphNetwork)$name,      # node names\n",
    "  vertex.label.cex = 1 # font size of node names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5c647",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "We are now going to perform a sentiment analysis on the lyrics of the songs.\n",
    "At [this link][https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm), you\n",
    "can download NRC emotion lexicons in different languages. It allows you to\n",
    "catalogue words in positive / negative classes and even emotions, as we will see\n",
    "later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd52f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lexicon <- read.table(\"resources/Italian-NRC-EmoLex.txt\",\n",
    "                                header = TRUE, sep = \"\\t\")\n",
    "sentiment_lexicon_corpus <- sentiment_lexicon %>% filter(Italian.Word %in% colnames(DTM))\n",
    "positive_terms <- sentiment_lexicon_corpus %>% filter(positive == 1) %>%\n",
    "  select(Italian.Word) %>% pull()\n",
    "negative_terms <- sentiment_lexicon_corpus %>% filter(positive == 0) %>%\n",
    "  select(Italian.Word) %>% pull()\n",
    "counts_positive <- rowSums(DTM[, positive_terms])\n",
    "counts_negative <- rowSums(DTM[, negative_terms])\n",
    "counts_all_terms <- rowSums(DTM)\n",
    "relative_sentiment_frequencies <- data.frame(\n",
    "  positive = counts_positive / counts_all_terms,\n",
    "  negative = counts_negative / counts_all_terms\n",
    ")\n",
    "sentiments_by_album <- aggregate(relative_sentiment_frequencies,\n",
    "                                 by = list(album = songs$album), mean)\n",
    "\n",
    "head(sentiments_by_album)\n",
    "df_sentiment <- melt(sentiments_by_album, id.vars = \"album\")\n",
    "ggplot(data = df_sentiment, aes(x = album, y = value, fill = variable)) + \n",
    "  geom_bar(stat=\"identity\", position=\"stack\") + coord_flip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124227e2",
   "metadata": {},
   "source": [
    "Here we have catalogued every single word, then grouped words by albums and\n",
    "visualized the sentiment distribution for each album. We see that the sentiment\n",
    "is mostly negative for every single one. It has probably to do with the topics\n",
    "that appear in Caparezza's songs, who is notoriously a socially engaged singer\n",
    "and therefore often criticizes the hypocrisy of society.\n",
    "\n",
    "It would be interesting to know if there is at least one song with more positive\n",
    "words than negative ones. Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_songs <- aggregate(\n",
    "  relative_sentiment_frequencies, by = list(album = songs$title),\n",
    "  mean) %>% filter(positive > negative)\n",
    "positive_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65617fd",
   "metadata": {},
   "source": [
    "Here they are. *Chi se ne frega della musica*, *FugadÃ * and *Uomini di molta fede*\n",
    "have been classified as mostly positive songs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b703f",
   "metadata": {},
   "source": [
    "## Emotion analysis\n",
    "\n",
    "For the last part of our analysis, we are going to look at the emotions,\n",
    "specifically anger, fear, joy and sadness, using the same NCR lexicon as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f86420",
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_terms <- sentiment_lexicon_corpus %>% filter(anger == 1) %>%\n",
    "  select(Italian.Word) %>% pull()\n",
    "fear_terms <- sentiment_lexicon_corpus %>% filter(fear == 1) %>%\n",
    "  select(Italian.Word) %>% pull()\n",
    "joy_terms <- sentiment_lexicon_corpus %>% filter(joy == 1) %>%\n",
    "  select(Italian.Word) %>% pull()\n",
    "sadness_terms <- sentiment_lexicon_corpus %>% filter(sadness == 1) %>%\n",
    "  select(Italian.Word) %>% pull()\n",
    "\n",
    "counts_anger <- rowSums(DTM[, anger_terms])\n",
    "counts_fear <- rowSums(DTM[, fear_terms])\n",
    "counts_joy <- rowSums(DTM[, joy_terms])\n",
    "counts_sadness <- rowSums(DTM[, sadness_terms])\n",
    "\n",
    "relative_emotion_frequencies <- data.frame(\n",
    "  anger = counts_anger / counts_all_terms,\n",
    "  fear = counts_fear / counts_all_terms,\n",
    "  joy = counts_joy / counts_all_terms,\n",
    "  sadness = counts_sadness / counts_all_terms\n",
    ")\n",
    "\n",
    "emotions_by_album <- aggregate(relative_emotion_frequencies,\n",
    "                                 by = list(album = songs$album), mean)\n",
    "\n",
    "head(emotions_by_album)\n",
    "\n",
    "df_emotions <- melt(emotions_by_album, id.vars = \"album\")\n",
    "ggplot(data = df_emotions, aes(x = album, y = value, fill = variable)) + \n",
    "  geom_bar(stat=\"identity\", position=\"stack\") + coord_flip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d21fe",
   "metadata": {},
   "source": [
    "Anger and fear are the prevailing emotions and they are mostly noticeable in\n",
    "the *Museica* album. The album where joy shares a larger percentage is *Exuvia*."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,name,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
